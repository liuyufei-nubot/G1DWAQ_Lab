{
  "created_at": 1768495564.4866066,
  "first_saved_iter": 0,
  "train_cfg": {
    "seed": 42,
    "device": "cuda:0",
    "num_steps_per_env": 24,
    "max_iterations": 10000,
    "empirical_normalization": false,
    "obs_groups": {},
    "clip_actions": null,
    "save_interval": 100,
    "experiment_name": "g1_dwaq",
    "run_name": "",
    "logger": "tensorboard",
    "neptune_project": "leggedlab",
    "wandb_project": "g1_dwaq",
    "resume": false,
    "load_run": ".*",
    "load_checkpoint": "model_.*.pt",
    "class_name": "OnPolicyRunner",
    "policy": {
      "class_name": "ActorCritic_DWAQ",
      "init_noise_std": 1.0,
      "noise_std_type": "scalar",
      "state_dependent_std": false,
      "actor_obs_normalization": {},
      "critic_obs_normalization": {},
      "actor_hidden_dims": [
        512,
        256,
        128
      ],
      "critic_hidden_dims": [
        512,
        256,
        128
      ],
      "activation": "elu",
      "cenet_out_dim": 19
    },
    "algorithm": {
      "class_name": "DWAQPPO",
      "num_learning_epochs": 5,
      "num_mini_batches": 4,
      "learning_rate": 0.001,
      "schedule": "adaptive",
      "gamma": 0.99,
      "lam": 0.95,
      "entropy_coef": 0.01,
      "desired_kl": 0.01,
      "max_grad_norm": 1.0,
      "value_loss_coef": 1.0,
      "use_clipped_value_loss": true,
      "clip_param": 0.2,
      "normalize_advantage_per_mini_batch": false,
      "rnd_cfg": null,
      "symmetry_cfg": null
    },
    "runner_class_name": "DWAQOnPolicyRunner"
  },
  "env_meta": {
    "num_envs": 4096,
    "num_obs": 100,
    "num_obs_hist": 5,
    "num_privileged_obs": 311,
    "num_actions": 29
  }
}