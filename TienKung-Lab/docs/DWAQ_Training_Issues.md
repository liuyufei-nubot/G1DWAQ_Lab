# DWAQ è®­ç»ƒé—®é¢˜è¯Šæ–­ä¸ä¿®å¤

## ğŸ”´ é—®é¢˜è¯Šæ–­å®Œæˆ

### è®­ç»ƒæ•°æ®å¯¹æ¯” (1200 iterations)

| æŒ‡æ ‡ | DWAQ (ä¿®å¤å‰) | æ™®é€š AC (åŒæœŸ) | æ™®é€š AC (æ”¶æ•›) |
|------|-------------|-------------|-------------|
| **mean_reward** | -7.3 | -9.0 | +0.4 |
| **episode_length** | 55 æ­¥ | 50 æ­¥ | 912 æ­¥ |
| **autoencoder_loss** | **0.95 (ä¸ä¸‹é™!)** | N/A | N/A |

### ğŸš¨ å‘ç°çš„æ ¹æœ¬åŸå› 

#### 1. â­â­â­â­â­ å…³é”® DWAQ Rewards è¢«æ³¨é‡Šæ‰

ç»ä»£ç åˆ†æå‘ç°ï¼Œ**4ä¸ªåŸç‰ˆ DreamWaQ æ ¸å¿ƒ reward å…¨éƒ¨è¢«æ³¨é‡Šæ‰**ï¼š

```python
# âŒ alive = RewTerm(func=mdp.alive, weight=0.15)
# âŒ gait_phase_contact = RewTerm(..., weight=0.18)  
# âŒ feet_swing_height = RewTerm(..., weight=-0.2)
# âŒ base_height = RewTerm(..., weight=-1.0)
```

**å½±å“åˆ†æï¼š**
- **æ—  `alive` å¥–åŠ±** â†’ æœºå™¨äººä¸çŸ¥é“"å­˜æ´»"æ˜¯å¥½äº‹ï¼Œæ— æ³•å­¦ä¹ ç»´æŒå¹³è¡¡
- **æ—  `gait_phase_contact`** â†’ æ— æ³•å­¦ä¹ æ­£ç¡®çš„ä¸¤è¶³äº¤æ›¿æ­¥æ€
- **æ—  `feet_swing_height`** â†’ æ‘†è…¿åŠ¨ä½œæ··ä¹±ï¼Œæ— æ³•å½¢æˆç¨³å®šæ­¥æ€
- **æ—  `base_height`** â†’ é‡å¿ƒæ§åˆ¶å¤±è´¥ï¼Œå®¹æ˜“æ‘”å€’

**ä¸ºä»€ä¹ˆè¿™å¯¼è‡´ autoencoder_loss ä¸ä¸‹é™ï¼Ÿ**

DWAQ çš„ Î²-VAE ç¼–ç å™¨éœ€è¦ä»**æˆåŠŸçš„è¡Œèµ°ç»éªŒ**ä¸­å­¦ä¹ æ½œåœ¨çŠ¶æ€ã€‚ä½†æ˜¯ï¼š
```
æ— å­˜æ´»å¥–åŠ± â†’ æœºå™¨äººå¿«é€Ÿæ‘”å€’ (55æ­¥)
â†’ æ— æœ‰æ•ˆè¡Œèµ°æ•°æ®
â†’ VAE æ— æ³•å­¦ä¹ é€Ÿåº¦å’Œåœ°å½¢ç‰¹å¾
â†’ autoencoder_loss åœåœ¨ 0.95 (éšæœºå™ªå£°æ°´å¹³)
```

#### 2. âš ï¸ åœ°å½¢è¿‡éš¾ (70% å°é˜¶)

```python
# åŸé…ç½®
DWAQ_TERRAINS_CFG:
  ä¸Šå°é˜¶: 35%
  ä¸‹å°é˜¶: 35%  
  å…¶ä»–: 30%
  æ€»å°é˜¶æ¯”ä¾‹: 70%
```

**é—®é¢˜ï¼š**
- åˆæœŸæœºå™¨äººåœ¨å°é˜¶ä¸Šå¿«é€Ÿæ‘”å€’
- æ— æ³•è·å¾—è¶³å¤Ÿçš„å¹³åœ°è¡Œèµ°ç»éªŒ
- VAE ç¼–ç å™¨ç¼ºå°‘åŸºç¡€è®­ç»ƒæ•°æ®

#### 3. âš ï¸ åˆå§‹å™ªå£°åä½ (0.8)

```python
init_noise_std = 0.8  # ä½äºæ ‡å‡†å€¼ 1.0
```

**å½±å“ï¼š**
- æ—©æœŸæ¢ç´¢ä¸è¶³
- éš¾ä»¥å‘ç°æœ‰æ•ˆåŠ¨ä½œ
- æ”¶æ•›é€Ÿåº¦æ…¢

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1: å¯ç”¨æ‰€æœ‰å…³é”® DWAQ Rewards

**æ–‡ä»¶:** `legged_lab/envs/g1/g1_dwaq_config.py`

```python
# âœ… å·²ä¿®å¤
alive = RewTerm(func=mdp.alive, weight=0.15)

gait_phase_contact = RewTerm(
    func=mdp.gait_phase_contact,
    weight=0.18,
    params={"sensor_cfg": SceneEntityCfg("contact_sensor", body_names=".*ankle_roll.*"), 
            "stance_threshold": 0.55},
)

feet_swing_height = RewTerm(
    func=mdp.feet_swing_height,
    weight=-0.2,
    params={
        "sensor_cfg": SceneEntityCfg("contact_sensor", body_names=".*ankle_roll.*"),
        "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_roll.*"),
        "target_height": 0.08,
    },
)

base_height = RewTerm(
    func=mdp.base_height,
    weight=-1.0,
    params={"asset_cfg": SceneEntityCfg("robot", body_names=".*torso.*"), "target_height": 0.98},
)
```

### ä¿®å¤ 2: é™ä½åœ°å½¢éš¾åº¦

**æ–‡ä»¶:** `legged_lab/terrains/terrain_generator_cfg.py`

```python
# âœ… å·²ä¿®å¤ - ä¿®æ”¹ä¸ºæ¸è¿›å¼åœ°å½¢
DWAQ_TERRAINS_CFG:
  å¹³åœ°: 25%      # æ–°å¢ï¼Œä¾¿äºåˆæœŸå­¦ä¹ 
  ä¸Šå°é˜¶: 20%    # ä» 35% é™ä½
  ä¸‹å°é˜¶: 20%    # ä» 35% é™ä½
  ç®€å•æ–œå¡: 25%  # å¢åŠ 
  å…¶ä»–: 10%
  æ€»å°é˜¶æ¯”ä¾‹: 40%  # ä» 70% é™ä½
```

**å¥½å¤„ï¼š**
- åˆæœŸæœ‰è¶³å¤Ÿå¹³åœ°ç»ƒä¹ åŸºç¡€æ­¥æ€
- é™ä½å°é˜¶éš¾åº¦ï¼Œå‡å°‘å¿«é€Ÿæ‘”å€’
- Curriculum learning ä¼šé€æ­¥å¢åŠ éš¾åº¦

### ä¿®å¤ 3: å¢åŠ åˆå§‹å™ªå£°

**æ–‡ä»¶:** `legged_lab/envs/g1/g1_dwaq_config.py`

```python
# âœ… å·²ä¿®å¤
self.policy.init_noise_std = 1.0  # ä» 0.8 å¢åŠ åˆ° 1.0
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®å¤åçš„è®­ç»ƒåº”è¯¥è¡¨ç°ä¸ºï¼š

| é˜¶æ®µ | Iterations | é¢„æœŸç°è±¡ |
|------|-----------|---------|
| **VAE å¿«é€Ÿå­¦ä¹ ** | 0-200 | `autoencoder_loss` ä» 0.95 å¿«é€Ÿä¸‹é™åˆ° 0.1-0.3 |
| **åŸºç¡€æ­¥æ€** | 200-500 | `episode_length` ä» 55 å¢é•¿åˆ° 200+ |
| **æ­¥æ€ä¼˜åŒ–** | 500-1500 | `mean_reward` ä» -7 æå‡åˆ° -2 å·¦å³ |
| **å°é˜¶è®­ç»ƒ** | 1500-3000 | curriculum å¢åŠ å°é˜¶æ¯”ä¾‹ï¼Œå­¦ä¹ çˆ¬æ¥¼ |
| **æ”¶æ•›** | 3000+ | `episode_length` è¾¾åˆ° 800+ï¼Œèƒ½ç¨³å®šçˆ¬å°é˜¶ |

### å…³é”®ç›‘æ§æŒ‡æ ‡

```python
# åœ¨ TensorBoard ä¸­é‡ç‚¹å…³æ³¨ï¼š
Loss/autoencoder       # åº”åœ¨ 200 iter å†…å¿«é€Ÿä¸‹é™
Train/mean_reward      # åº”é€æ­¥ä¸Šå‡
Train/mean_episode_length  # åº”é€æ­¥å¢é•¿
Curriculum/terrain_levels  # åœ°å½¢éš¾åº¦é€æ­¥å¢åŠ 
```

---

## ğŸ¯ é‡æ–°è®­ç»ƒæ­¥éª¤

### 1. åˆ é™¤æ—§æ•°æ®

```bash
cd /home/lyf/code/geoloco/TienKung-Lab
rm -rf logs/g1_dwaq/*  # åˆ é™¤æ‰€æœ‰æ—§ checkpoint
```

**é‡è¦:** ç”±äºä¿®æ”¹äº† reward é…ç½®ï¼Œæ—§çš„ checkpoint ä¸å…¼å®¹ï¼

### 2. å¯åŠ¨è®­ç»ƒ

```bash
python legged_lab/scripts/train.py \
    --task=g1_dwaq \
    --headless \
    --num_envs=4096 \
    --max_iterations=5000
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# æ‰“å¼€ TensorBoard
tensorboard --logdir=logs/g1_dwaq
```

**é‡ç‚¹è§‚å¯Ÿ:**
- **å‰ 200 iterations**: `autoencoder_loss` å¿…é¡»å¿«é€Ÿä¸‹é™ï¼
  - å¦‚æœä»ç„¶ä¸ä¸‹é™ â†’ è¯´æ˜è¿˜æœ‰å…¶ä»–é—®é¢˜
- **å‰ 500 iterations**: `episode_length` åº”è¯¥å¼€å§‹å¢é•¿
  - å¦‚æœå§‹ç»ˆå¾ˆçŸ­ â†’ æ£€æŸ¥ reward æƒé‡

### 4. è°ƒè¯•å»ºè®®

å¦‚æœè®­ç»ƒ 500 iterations åä»ç„¶æ²¡æœ‰æ”¹å–„ï¼š

```python
# å¯ä»¥å°è¯•è°ƒæ•´è¿™äº›å‚æ•°ï¼š

# 1. å¢åŠ å­˜æ´»å¥–åŠ±æƒé‡
alive.weight = 0.3  # ä» 0.15 å¢åŠ 

# 2. é™ä½æƒ©ç½šé¡¹æƒé‡
termination_penalty.weight = -100  # ä» -200 é™ä½
feet_slide.weight = -0.1  # ä» -0.25 é™ä½

# 3. å¢åŠ å¹³åœ°æ¯”ä¾‹
flat.proportion = 0.4  # ä» 0.25 å¢åŠ 

# 4. é™ä½å­¦ä¹ ç‡
self.algorithm.learning_rate = 5e-4  # ä» 1e-3 é™ä½
```

---

## ğŸ“‹ ä¿®å¤æ¸…å•

- [x] å¯ç”¨ `alive` reward  
- [x] å¯ç”¨ `gait_phase_contact` reward
- [x] å¯ç”¨ `feet_swing_height` reward
- [x] å¯ç”¨ `base_height` reward
- [x] é™ä½åœ°å½¢éš¾åº¦ (70% â†’ 40% å°é˜¶)
- [x] å¢åŠ å¹³åœ°æ¯”ä¾‹ (0% â†’ 25%)
- [x] å¢åŠ åˆå§‹å™ªå£° (0.8 â†’ 1.0)
- [ ] ç­‰å¾…è®­ç»ƒç»“æœéªŒè¯

---

## ğŸ” å…¶ä»–æ½œåœ¨é—®é¢˜ (å¾…éªŒè¯)

å¦‚æœä¿®å¤åä»ç„¶æœ‰é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥ï¼š

### 1. obs_dim å‚æ•°ä¼ é€’

åœ¨ `dwaq_on_policy_runner.py` ä¸­ï¼Œæ£€æŸ¥ DWAQ PPO åˆå§‹åŒ–ï¼š

```python
# éœ€è¦ç¡®è®¤ obs_dim æ˜¯å¦æ­£ç¡®
self.alg = DWAQPPO(..., obs_dim=66)  # åº”è¯¥æ˜¯ actor_obs çš„ç»´åº¦
```

**éªŒè¯æ–¹æ³•:**
```python
# åœ¨ç¯å¢ƒä¸­æ‰“å°è§‚æµ‹ç»´åº¦
print(f"Actor obs shape: {obs.shape}")  # åº”è¯¥æ˜¯ [num_envs, 66]
print(f"Critic obs shape: {critic_obs.shape}")  # åº”è¯¥æ˜¯ [num_envs, 307]
```

### 2. é€Ÿåº¦ç›‘ç£ä¿¡å·

VAE éœ€è¦ä» `prev_critic_obs` æå–é€Ÿåº¦ï¼š

```python
vel_target = prev_critic_obs[:, obs_dim:obs_dim+3]
# obs_dim=66 æ—¶ï¼Œåº”è¯¥æå– [66:69]ï¼Œå³å‰3ç»´é€Ÿåº¦
```

**å¦‚æœ obs_dim ä¸å¯¹ï¼Œé€Ÿåº¦ç›‘ç£ä¼šå¤±è´¥ï¼**

### 3. Beta é€€ç«ç­–ç•¥

åŸç‰ˆå¯èƒ½ä½¿ç”¨ beta é€€ç«ï¼Œä½†è¿™ä¸æ˜¯å¿…é¡»çš„ã€‚å¦‚æœéœ€è¦ï¼š

```python
# åœ¨ runner ä¸­æ·»åŠ  beta schedule
beta = min(1.0, current_iter / 1000)  # ä» 0 é€æ­¥å¢åŠ åˆ° 1
loss_dict = self.alg.update(beta=beta)
```

---

## æ€»ç»“

**æ ¸å¿ƒé—®é¢˜:** ç¼ºå°‘å…³é”® DWAQ rewards å¯¼è‡´æœºå™¨äººæ— æ³•å­¦ä¹ å­˜æ´»å’Œæ­¥æ€ï¼Œè¿›è€Œå¯¼è‡´ VAE ç¼–ç å™¨æ— æ³•ä»æœ‰æ•ˆæ•°æ®ä¸­å­¦ä¹ ã€‚

**ä¿®å¤æ•ˆæœé¢„æœŸ:** å¯ç”¨ rewards åï¼Œæœºå™¨äººåº”è¯¥èƒ½åœ¨ 200 iter å†…å­¦ä¼šåŸºç¡€å­˜æ´»ï¼ŒVAE loss å¿«é€Ÿä¸‹é™ï¼Œç„¶åé€æ­¥å­¦ä¹ è¡Œèµ°å’Œçˆ¬å°é˜¶ã€‚

**å¦‚æœè¿˜æœ‰é—®é¢˜ï¼Œè¯·æä¾›:**
1. ä¿®å¤åçš„è®­ç»ƒæ—¥å¿— (å‰ 500 iterations)
2. TensorBoard æˆªå›¾
3. `autoencoder_loss` çš„å…·ä½“æ•°å€¼å˜åŒ–

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

